---
title: "Midterm Project"
author: "Yang Kang Chua, Shen, Tong"
date: "March 3, 2023"
format:
  html:
    code-fold: true
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
jupyter: python3
---
## Data Cleaning
```{python}
import numpy as np
import pandas as pd

pd.set_option('display.max_columns', None)

nyc311 = pd.read_csv("nyc311_011523-012123_by022023.csv")

nyc311.head()

```

Get a statistical summary

```{python}
nyc311.describe()
```

Looks good! Lets take a look at the number of missing values.

```{python}
nyc311.isna().sum()
```

Get the type of the data frame
```{python}
nyc311.info();
```
Check missing value percentage

```{python}
# Find missing value
missing_value = nyc311.isnull()

#Divide the full data length to get the percentage
missing_value = missing_value.sum()/len(nyc311)

#Convert into dataframe
missing_value = missing_value.to_frame()

#Name the column
missing_value.columns = ['Missing Percentage']

#Display
missing_value

```
Convert the time to date time format.

```{python}
nyc311['Created Date'] = pd.to_datetime(nyc311['Created Date'], errors='coerce')
nyc311['Closed Date'] = pd.to_datetime(nyc311['Closed Date'], errors='coerce')
```
Let take a look on if `Closed Date` earlier than `Created Date`.

```{python}
# Create a filter to find if the Closed Date is earlier and Created Date

Filter_1 = nyc311[nyc311['Created Date'] > nyc311['Closed Date']]

Filter_1.head()
```

Few thing we can do to clean the data:

* Remove the `Closed Date` earlier than `Created Date`.
* Missing Value from `Incident Zip` can be found using `Latitude` and `Longitude` using `uszipcode`.
* `Latitude` and `Longitude` can be found using the available `Incident Address`, `Street Name`, `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`, `Intersection Street 2` or as known as `Address Type`.
* We can get `X Coordinate (State Plane)`and `Y Coordinate (State Plane)` using `stateplane`.
* Update `Borough` from `Incident Zip`.

Begin by remove the `Closed Date` earlier than `Created Date``.

```{python}
nyc311 = nyc311[nyc311['Created Date'] < nyc311['Closed Date']]

```

Now work on getting `Latitude` and `Longitude` can be found using the available `Incident Address`, `Street Name`, `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`, `Intersection Street 2` or as known as `Address Type`.

```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

pd.crosstab(Filter_1["Incident Address"].isnull(), Filter_1["Latitude"].isnull())
pd.crosstab(Filter_1["Street Name"].isnull(), Filter_1["Latitude"].isnull())
```

Begin with solving `Address Type` that is `INTERSECTION`.

```{python}
# Get the remaining data that is missing with Latitude
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

# Get the data that contain INTERSECTION
Filter_1 = Filter_1[Filter_1['Address Type'] =='INTERSECTION']

from geopy.geocoders import Nominatim
from geopy.geocoders import GoogleV3
from geopy.extra.rate_limiter import RateLimiter

Intersection = Filter_1['Intersection Street 1'] + ' @ ' + Filter_1['Intersection Street 2'] + ' NY'

for index in Intersection.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Intersection[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]
        
# Update the data
for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]

```
Then work with address type `BLOCKFACE`.
```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1 = Filter_1[Filter_1['Address Type'] == 'BLOCKFACE']

Cross_Street = Filter_1['Cross Street 1'] + ' @ ' + Filter_1['Cross Street 2'] + ' NY '

for index in Cross_Street.index:
    if not Filter_1['Cross Street 1'][index] == Filter_1['Cross Street 2'][index]:
        if Filter_1['Cross Street 2'][index] is not np.nan:
            if Filter_1['Cross Street 2'][index] != 'SEE COMMENTS FOR CROSS ST':
                geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
                location = geocode(Cross_Street[index], timeout = 10)
                if location is not None:
                    Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

# Use the Street Name and Cross Street 1 to get the latitude and longitude
Filter_2 = Filter_1[Filter_1['Latitude'].isnull()]

Street_Cross = Filter_2['Street Name'] + ' @ ' + Filter_2['Cross Street 1'] + ' NY '

for index in Street_Cross.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Street_Cross[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]
```

Work on address type `ADDRESS`.
```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]
Filter_1 = Filter_1[Filter_1['Address Type'] == 'ADDRESS']

Address = Filter_1['Incident Address'] + ' NY '

for index in Address.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Address[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

# Update
for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]


```
Solve the remaining address.
```{python}
# Solve the remaining address

Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1 = Filter_1[Filter_1['Incident Address'].notnull()]

Address = Filter_1['Incident Address'] + ' NY '

for index in Address.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Address[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]

```
These are the remaining latitude and longitude that is missing
```{python}
# Show the remaining data that has missing Latitude and Longitude

Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1
```

Most of the latitude are due to most location is at hightway.

Now work on getting zipcode from available `latitude` and `longitude`.

```{python}
from uszipcode import SearchEngine

# Create a SearchEngine object
search = SearchEngine()

Filter_1 = nyc311[nyc311['Incident Zip'].isnull()]

Filter_1 = Filter_1[Filter_1['Latitude'].notnull()]

for index in Filter_1.index:
    z = search.by_coordinates(Filter_1['Latitude'][index],Filter_1['Longitude'][index], radius = 25)
    Filter_1.loc[index,'Incident Zip'] = z[0].zipcode

# Make an update
for index in Filter_1.index:
    nyc311.loc[index,'Incident Zip']   = Filter_1['Incident Zip'][index]

```

The remaining missing date still shown here, this means that the rest of the `Incident Zip` is filled!

```{python}
Filter_1 = nyc311[nyc311['Incident Zip'].isnull()]

Filter_1
```

Update `Borough` from `Incident Zip`.

```{python}
# Function that classified the borough
def nyczip2burough(zip):
    nzip = int(zip)
    if nzip >= 10001 and nzip <= 10282:
        return "MANHATTAN"
    elif nzip >= 10301 and nzip <= 10314:
        return "STATEN ISLAND"
    elif nzip >= 10451 and nzip <= 10475:
        return "BRONX"
    elif nzip >= 11004 and nzip <= 11109:
        return "QUEENS"
    elif nzip >= 11351 and nzip <= 11697:
        return "QUEENS"
    elif nzip >= 11201 and nzip <= 11256:
        return "BROOKLYN"
    else:
        return np.nan

Filter_1 = nyc311[nyc311['Borough'] == 'Unspecified']

Filter_1 = Filter_1[Filter_1['Incident Zip'].notnull()]

for index in Filter_1.index:
    burough = nyczip2burough(Filter_1.loc[index,['Incident Zip']])
    Filter_1.loc[index,'Borough'] = burough

#Update the data
for index in Filter_1.index:
  nyc311.loc[index,'Borough']   = Filter_1['Borough'][index]
```

Get `X Coordinate (State Plane)`and `Y Coordinate (State Plane)` using `stateplane`.

```{python}
import stateplane

Filter_1 = nyc311[nyc311['X Coordinate (State Plane)'].isnull()]

Filter_1 = Filter_1[Filter_1['Latitude'].notnull()]

for index in Filter_1.index:
    x, y = stateplane.from_lonlat(Filter_1['Longitude'][index],Filter_1['Latitude'][index])
    Filter_1.loc[index,'X Coordinate (State Plane)'] = x
    Filter_1.loc[index,'Y Coordinate (State Plane)'] = y

for index in Filter_1.index:
    nyc311.loc[index,'X Coordinate (State Plane)'] = Filter_1['X Coordinate (State Plane)'][index]
    nyc311.loc[index,'Y Coordinate (State Plane)'] = Filter_1['Y Coordinate (State Plane)'][index]

#Show the filled state plane 
Filter_1
```
### Summary after cleaning the data.

* There are more missing `X Coordinate (State Plane)` than `Y Coordinate (State Plane)`.
* There is duplicate when stating `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`,and  `Intersection Street 2`.
* Agency from `DOT` seems commonly mixed up the `Closed Date` and `Created Date`.
* `Borough` that is `unspecified` should count as `missing`.
* Many `Park Facility Name` is `unspecified` should count as `missing`. 

## Explore duration distribution
Remove the request not from NYPD

```{python}
nyc311_NYPD = nyc311[nyc311['Agency'] == 'NYPD']

A = nyc311_NYPD[nyc311_NYPD['Closed Date'].isnull()]

A
```
Only one data still in progress. We can remove that too. Add a new variable called `Duration (hours)`

```{python}
nyc311_NYPD = nyc311_NYPD[nyc311_NYPD['Closed Date'].notnull()]

nyc311_NYPD['Duration (hours)'] = nyc311_NYPD['Closed Date'] - nyc311_NYPD['Created Date']

col = nyc311_NYPD.pop("Duration (hours)")

s_hours = col.dt.total_seconds() / 3600.0

nyc311_NYPD.insert(3,s_hours.name ,s_hours)

nyc311_NYPD['Day type'] = nyc311_NYPD['Created Date'].dt.dayofweek

for index in nyc311_NYPD.index:
    if nyc311_NYPD['Day type'][index] < 5:
       nyc311_NYPD.loc[index,'Day type'] = 'Weekday'
    else:
       nyc311_NYPD.loc[index,'Day type'] = 'Weekend'
       
col = nyc311_NYPD.pop("Day type")

nyc311_NYPD.insert(4,col.name ,col)

nyc311_NYPD
```

### Visualize the distribution of uncensored duration by weekdays/weekend and by borough.

Lets check the Borough that is `Unspecified`.

```{python}
# Remove the unspecified Borough
A = nyc311_NYPD[ nyc311_NYPD['Borough'] == 'Unspecified']

A
```

The remaining borough that is `Unspecified` does not have `Incident Zip` which merging with uszipcode does not help to get more info about the data. 

```{python}
# Remove the unspecified Borough
nyc311_NYPD = nyc311_NYPD[ nyc311_NYPD['Borough'] != 'Unspecified']

# Remove the zipcode that is null due to missing latitude and longitude
nyc311_NYPD = nyc311_NYPD[nyc311_NYPD['Incident Zip'].notnull()]

from plotnine import *

(
    ggplot(nyc311_NYPD, aes(x='Borough',y = 'Duration (hours)', fill='Day type'))
    + geom_boxplot()
)

```

Test whether the distributions are the same across weekdays/weekends.

```{python}
from scipy.stats import mannwhitneyu

# Create the subset of data
weekday_dur = nyc311_NYPD.loc[nyc311_NYPD['Day type'] == 'Weekday', 'Duration (hours)']

weekend_dur = nyc311_NYPD.loc[nyc311_NYPD['Day type'] == 'Weekend', 'Duration (hours)']

# Perform the weekday/weekend comparison
weekday_weekend_pvalue = mannwhitneyu(weekday_dur, weekend_dur).pvalue

print(weekday_weekend_pvalue)
```
The pvalue shows that is diffeent accross weekdays and weekends.

Test whether the distributions are the same across borough.

```{python}
# Create the subset of the data
from scipy.stats import kruskal

group1 =nyc311_NYPD[nyc311_NYPD['Borough'] == 'BROOKLYN']['Duration (hours)']
group2 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'QUEENS']['Duration (hours)']
group3 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'MANHATTAN']['Duration (hours)']
group4 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'BRONX']['Duration (hours)']
group5 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'STATEN ISLAND']['Duration (hours)']

statistic, p_value = kruskal(group1, group2, group3, group4, group5)

print('Borough p-values:', p_value)
```
The pvalue shows that the distribution are different across across borough. 

Lets merge with uszipcode data.

```{python}
import os
# set the default database file location
db_file = os.path.abspath("simple_db.sqlite")

import sqlite3
import pandas as pd
# change to your own path after installing uszipcode
con = sqlite3.connect(db_file)
zipdf = pd.read_sql_query("SELECT * from simple_zipcode", con)
zipdf.info()

# Change the type to float64
zipdf['zipcode'] = zipdf['zipcode'].astype('float64')

# Change the type to float64 too
nyc311_NYPD['Incident Zip'] = nyc311_NYPD['Incident Zip'].astype('float64')

merged_df = pd.merge(nyc311_NYPD, zipdf, how = 'left', left_on = 'Incident Zip', right_on = 'zipcode')

# Drop repeating data
merged_df = merged_df.drop(['zipcode','lat','lng'],axis = 1)
```

Displaying merged data.

```{python}
merged_df

```

## Model Selection

### Importing new variable
Lets choose a model to predict `duration`.

```{python}
import pandas as pd
import numpy as np

# load data 
train = pd.read_csv("nyc311_NYPD_merged.csv")
test = pd.read_csv("nyc311_NYPD_test_merged.csv")

# Create new variable over3h.
train['over3h'] = train['Duration (hours)'].\
    apply(lambda x: 1 if x >= 3 else 0)
test['over3h'] = test['Duration (hours)'].\
    apply(lambda x: 1 if x >= 3 else 0)
```
Construct a `hour` variable with integer values from 0 to 23.
```{python}
train['hour'] = pd.to_datetime(train['Created Date']).dt.hour
test['hour'] = pd.to_datetime(test['Created Date']).dt.hour
```
Construct a `house_income_ratio` represent the ratio of `median_home_value` to `median_household_income`.
```{python}
train['house_income_ratio'] = train['median_home_value'] / train['median_household_income']
test['house_income_ratio'] = test['median_home_value'] / train['median_household_income']
```
Drop cases with missing values for certain columns.
```{python}
train = train.dropna(subset=['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type','population_density',\
                 'median_home_value', 'house_income_ratio'])
test = test.dropna(subset=['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type','population_density',\
                 'median_home_value', 'house_income_ratio'])
```

When using the fitted model to predict the test data, we noticed that there are certain features that appear only in the testing data (3 cases) and certain features that appear only in the training data (6 cases). Therefore, we need to revisit this step and drop these cases before proceeding with the fit and prediction.

Drop cases with certain values for certain columns

```{python}
test = test.drop(test[test['Community Board'] == '28 BRONX'].index)
test = test.drop(test[test['Community Board'] == '80 QUEENS'].index)
test = test.drop(test[test['Complaint Type'] == 'Squeegee'].index)
train = train.drop(train[train['Community Board'] == '27 BRONX'].index)
train = train.drop(train[train['Community Board'] == '81 QUEENS'].index)
train = train.drop(train[train['Resolution Description'] == \
'Your complaint has been received by the Police Department and additional information will be available later.'].index)
train = train.drop(train[train['Open Data Channel Type'] == 'UNKNOWN'].index)
```
Get the training data and separate the categorical and continuous variables into separate dataframes.
```{python}

train_categorical_variables = train[['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type']]
train_continuous_variables = train[['population_density', 'median_home_value', 'house_income_ratio']]

# Perform one-hot encoding on the categorical variables
train_categorical_encoded = pd.get_dummies(train_categorical_variables)
# Combine the one-hot encoded categorical variables with the continuous variables
X_train = pd.concat([train_categorical_encoded, train_continuous_variables], axis=1)
print(X_train.shape) 
```
Get the testing data and separate the categorical and continuous variables into separate dataframes
```{python}
test_categorical_variables = test[['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type']]
test_continuous_variables = test[['population_density', 'median_home_value', 'house_income_ratio']]

# Perform one-hot encoding on the categorical variables
test_categorical_encoded = pd.get_dummies(test_categorical_variables)

# Combine the one-hot encoded categorical variables with the continuous variables
X_test = pd.concat([test_categorical_encoded, test_continuous_variables], axis=1)

print(X_test.shape)
```

```{python}
# Get y for both the training and test data
y_train = train['over3h'].values
y_test = test['over3h'].values
```

### Import Model and fit the Model
```{python}
from sklearn.svm import SVC

# Fit SVM
svm = SVC()
svm.fit(X_train, y_train)
```

```{python}
from sklearn import tree

# Fit decision tree
tree1 = tree.DecisionTreeClassifier()
tree1.fit(X_train, y_train)
```

```{python}
tree1.tree_.node_count
```

```{python}
# caculate the predicted values
svm_pred = svm.predict(X_test)
tree1_pred = tree1.predict(X_test)
```

```{python}
# evaluate the model with default parameters
from sklearn.metrics import confusion_matrix, \
accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Confusion matrix
svm_cm = confusion_matrix(y_test, svm_pred)
tree1_cm = confusion_matrix(y_test, tree1_pred)

# Accuracy
svm_acc = accuracy_score(y_test, svm_pred)
tree1_acc = accuracy_score(y_test, tree1_pred)

# Precision
svm_precision = precision_score(y_test, svm_pred, zero_division=1)
tree1_precision = precision_score(y_test, tree1_pred)

# Recall
svm_recall = recall_score(y_test, svm_pred)
tree1_recall = recall_score(y_test, tree1_pred)

# F1-score
svm_f1 = f1_score(y_test, svm_pred)
tree1_f1 = f1_score(y_test, tree1_pred)

# AUC
svm_auc = roc_auc_score(y_test, svm_pred)
tree1_auc = roc_auc_score(y_test, tree1_pred)
```

```{python}
print("SVM results:")
print("Confusion matrix:")
print(svm_cm)
print("Accuracy:", svm_acc)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1-score:", svm_f1)
print("AUC:", svm_auc)

print("decision tree results:")
print("Confusion matrix:")
print(tree1_cm)
print("Accuracy:", tree1_acc)
print("Precision:", tree1_precision)
print("Recall:", tree1_recall)
print("F1-score:", tree1_f1)
print("AUC:", tree1_auc)
```

### Summary on SVM model
We observed that the SVM model performed poorly with the default parameters. To improve its performance, we decided to scale the continuous variables and use cross-validation to find the optimal hyperparameters. However, the SVM model took too long to train and did not converge even after running for half an hour.

In contrast, the decision tree model provided accurate predictions with the default parameters and was much more efficient than the SVM model. Therefore, we decided to use the decision tree model for further optimazition.

Here are the syntax that tried to optimize svm parameters
```python
from sklearn.preprocessing import StandardScaler

# scale the continuous variables 

scaler = StandardScaler()
X_train_cont_scaled = scaler.fit_transform(train_continuous_variables)
X_test_cont_scaled = scaler.transform(test_continuous_variables)
X_train_cont_scaled = pd.DataFrame(X_train_cont_scaled, columns=['scaled1', 'scaled2', 'scaled3'])
X_test_cont_scaled = pd.DataFrame(X_test_cont_scaled, columns=['scaled1', 'scaled2', 'scaled3'])
train_categorical_encoded = train_categorical_encoded.reset_index()
test_categorical_encoded = test_categorical_encoded.reset_index()
X_train_s = pd.concat([train_categorical_encoded, X_train_cont_scaled], axis=1)
X_test_s = pd.concat([test_categorical_encoded, X_test_cont_scaled], axis=1)

# Fit SVM with scaled variables
svm2 = SVC()
svm2.fit(X_train_s, y_train)

# define the hyperparameter space to search over for svm
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.001, 0.01, 0.1],
              'gamma': [0.01, 0.1, 1, 'scale', 'auto'],
              'kernel': ['linear', 'rbf', 'sigmoid']}

# perform cross-validation with GridSearchCV
grid_search = GridSearchCV(svm2, param_grid, cv=5, scoring='f1')

# fit the GridSearchCV object to the training data
grid_search.fit(X_train_s, y_train)

# print the best hyperparameters found
print("Best hyperparameters: ", grid_search.best_params_)
```

```{python}
# define the hyperparameter grid 

from sklearn.model_selection import GridSearchCV

param_grid = {'criterion': ['gini', 'entropy'],
              'min_impurity_decrease': [0, 1e-5, 1e-4, 1e-3],
              'ccp_alpha': [0.0, 1e-5, 1e-4, 1e-3, 0.01, 0.1]}

# perform cross-validation with GridSearchCV
grid_search = GridSearchCV(tree1, param_grid, cv=5, scoring='roc_auc')

# fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# print the best hyperparameters found
grid_search.best_params_
```

```{python}
# Use parameters from cross-validation to train another model
tree2 = tree.DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.001, min_impurity_decrease=0)
tree2 = tree2.fit(X_train, y_train)
tree2.tree_.node_count
```

```{python}
# caculate the predicted values
tree2_pred = tree2.predict(X_test)

# evaluate the model 

# Confusion matrix
tree2_cm = confusion_matrix(y_test, tree2_pred)

# Accuracy
tree2_acc = accuracy_score(y_test, tree2_pred)

# Precision
tree2_precision = precision_score(y_test, tree2_pred)

# Recall
tree2_recall = recall_score(y_test, tree2_pred)

# F1-score
tree2_f1 = f1_score(y_test, tree2_pred)

# AUC
tree2_auc = roc_auc_score(y_test, tree2_pred)
```

```{python}
print("decision tree2 results:")
print("Confusion matrix:")
print(tree2_cm)
print("Accuracy:", tree2_acc)
print("Precision:", tree2_precision)
print("Recall:", tree2_recall)
print("F1-score:", tree2_f1)
print("AUC:", tree2_auc)
```

### Summary for Model Selection
Cross-validation was used to optimize the tuning parameters for building the second decision tree model with the objective of minimizing overfitting and maximizing AUC. While the second model (tree2) performed better on accuracy and precision, it did not perform as well as the first model (tree1) on other metrics such as recall, F1 score, and AUC. However, tree2 had significantly fewer nodes than tree1. Overall, both models performed adequately and the choice of model would depend on the specific metric that is more important for the task at hand. For instance, if higher precision is desired, then tree1 would be the preferred choice.


## Research question of interest and exploration

Our "research" question is what suggestions we can give for individuals (friends or clients) seeking to purchase a neighbor-friendly house in NYC. Our analysis involved exploring factors such as total complains, noise complaints, parking violations, NYPD location, and median house value. 

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
import pandas as pd
from plotnine import *
from plotnine.data import *
import numpy as np
%matplotlib inline
from shapely.geometry import Point 
```

```{python}
# Load data and select some columns
nyc_merged = pd.read_csv('nyc311_NYPD_merged.csv')
comp_df = nyc_merged[['Complaint Type', 'Incident Zip', 'Borough', 'Latitude', 'Longitude', 'median_home_value']].copy()
comp_df['comp_type'] = comp_df['Complaint Type'].astype('category').cat.codes
comp_df['zipstr'] = comp_df['Incident Zip'].apply(lambda f: str(int(f)))
comp_df
```

### Get an idea of complaint distribution - Plot complaint locations

To get a basic idea of complaint distribution, we can plot the locations of complaints reported in different boroughs. This will help us identify areas with a higher volume of complaints and any patterns or trends that may be specific to certain boroughs. We can do this by creating a map with different boroughs shown in different colors and using smaller markers to represent individual complaints. From the following map, we can see that Staten Island has the least complaint density.

```{python}
crs = 'epsg:4326' # coordinate reference system
comp_geometry = gpd.points_from_xy(comp_df['Longitude'], comp_df['Latitude'])
comp_gdf = gpd.GeoDataFrame(comp_df, crs=crs, geometry=comp_geometry)
comp_gdf.plot(column = 'Borough',markersize=0.5, legend=True)
```

While Staten Island may have a much lower complaint density than other boroughs, it may have certain disadvantages such as limited transportation options. For example, commuting from Staten Island to Manhattan may be time-consuming and inconvenient for those who need to work in Manhattan. Therefore, we continue to explore the complaint density in different zip codes.

```{python}
# Load new york borough boundary data
nybb_gdf = gpd.read_file(gpd.datasets.get_path('nybb'))
```

```{python}
# zipcodes = gpd.read_file('data/tl_2022_us_zcta520/tl_2022_us_zcta520.shp')
zipcodes = gpd.read_file('nyc_zipcode.geojson')
# Count total complaints per zip code
total_comp_df = comp_df.groupby('zipstr')['comp_type'].count().reset_index(name='total complaints')
total_comp_df
# merge it with zip code area and plot it on the map
merge_total_comps = zipcodes.merge(total_comp_df, left_on='GEOID20', right_on='zipstr')
comp_map = merge_total_comps.explore(column='total complaints', cmap='OrRd', legend=True)

comp_map = nybb_gdf.explore(m=comp_map, color='black', style_kwds={'fill': False})
comp_map
```

Based on the interactive map above (displaying complaint density for different zip codes), it would be advisable to avoid certain areas in Brooklyn and the Bronx.

## Explore certain complaint type per zip code

While we have explored some general neighbor-friendly areas by analyzing the total number of complaints, it's important to consider specific needs of individuals. For instance, if your friends/clients are sensitive to noise and prefer a quiet environment, or if they drive and dislike areas with high incidents of illegal parking, we need to take those specific needs into account when suggesting neighborhoods.

#### Noise-related issues

```{python}
comp_zipcode_df = comp_df.groupby('zipstr')['Complaint Type'].value_counts().\
reset_index(name='count')
# Get noise related complaints
noise_df = comp_zipcode_df[comp_zipcode_df['Complaint Type'].\
                           apply(lambda comp_type: comp_type.startswith('Noise'))].copy()
noise_df = noise_df.groupby('zipstr')['count'].sum().reset_index(name='noise complaints')
# merge it with zip code area and plot it on the map
merge_noise = zipcodes.merge(noise_df, left_on='GEOID20', right_on='zipstr')
noise_map = merge_noise.explore(column='noise complaints', cmap='OrRd', legend=True)

noise_map = nybb_gdf.explore(m=noise_map, color='black', style_kwds={'fill': False})
noise_map
```

Based on the noise complaint map, it is recommended to consider Staten Island (excluding the northeast area with zip code 10301) and most areas in Queens (avoide the western area) for those who are sensitive to noise. For those who  prefer a quieter location but want to live in Manhattan, the Upper East Side might be a good choice. 

#### Illegal parking

```{python}
# Get illegal parking data
illpark_df = comp_zipcode_df[comp_zipcode_df['Complaint Type'] == 'Illegal Parking'].copy()
# merge it with zip code area and plot it on the map
merge_illpark = zipcodes.merge(illpark_df, left_on='GEOID20', right_on='zipstr')
illpark_map = merge_illpark.explore(column='count', cmap='OrRd', legend=True)
illpark_map = nybb_gdf.explore(m=illpark_map, color='black', style_kwds={'fill': False})
illpark_map
```

This map shows that Staten Island and most areas in Queens (avoide the western area) are also good choices for people who need to drive and dislike illegal parking. In addition, Manhattan has relatively low numbers of illegal parking complaints.

### Exploring factors that might impact certain types of complaints

Now that we have found some larger areas that we might want to suggest based on different needs, such as Staten Island, the Upper East Sideand, or most areas in Queens. You may still want more specific suggestions for a good place to live. For example, you may be wondering if it is a good idea to live near NYPD as this could potentially reduce noise and illegal activities in the area. Or, you may be curious if higher house prices correlate with having more friendly neighbors (do you need to pay more for a more friendly neighborhood). Let's take a look.

#### Live near NYPD?

Load NYPD Precincts data
Source: precinct locations are looked up from [here](https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page)
Longitudes and latitudes are queried through Google Maps

Use this data to explore if 311 calls have any relationship with NYPD precinct locations

```{python}
# Add NYPD precinct locations
crs = {'init': 'epsg:4326'} # coordinate reference system
nypd_location = pd.read_csv('nypd_precinct_locations.csv')
nypd_geometry = gpd.points_from_xy(nypd_location['Longitude'], nypd_location['Latitude'])
nypd_gdf = gpd.GeoDataFrame(nypd_location, crs=crs, geometry=nypd_geometry)

nypd_gdf.explore(m = comp_map, marker_type='circle_marker', marker_kwds={'radius': 1}, legend=True)
```

From the eyeball, it is not easy to tell if there is any relationship between the complaint counts and the distance to NYPD. To investigate whether living near NYPD impacts the number of complaints, I created a histogram of the distance between complaint locations and the nearest police station as x axis and the number of complaints as y axis.

```{python}
# To calculate distance, need to use a coordinate reference system that preserves distance. Here we use UTM.
# reference: https://gis.ny.gov/coordinationprogram/workgroups/wg_1/related/standards/datum.htm
nyc_utm = '+proj=utm +zone=18 +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs'

comp_gdf2 = comp_gdf.to_crs(crs=nyc_utm) # Convert total compliants' GeoDataFrame
nypd_gdf2 = nypd_gdf.to_crs(crs=nyc_utm) # Covert NYPD precinct GeoDataFrame

# Calculate the distance to nearest precinct for each complaint
nearest_res = nypd_gdf2.sindex.nearest(comp_gdf2.geometry, return_all=False, return_distance=True)
dist_to_pd = nearest_res[1] # second element is the distance array

# View the data in a histogram
fig, ax = plt.subplots()
ax.hist(dist_to_pd, bins=100)
ax.set_xlabel('Distance to precinct (m)')
ax.set_ylabel('Number of complaints')
```

From this histogram, our suggestion is: don't specifically look for houses or apartments that near NYPD. 

#### Buy a more expensive house/apartment?

```{python}
# Get mean median home value for each zip code and drop NA values
median_homeval_df = comp_df.groupby('zipstr')['median_home_value'].mean().reset_index(name='median home value').dropna()

# merge it with zip code area and plot it on the map
merge_total_comps = zipcodes.merge(median_homeval_df, left_on='GEOID20', right_on='zipstr')
homeval_map = merge_total_comps.explore(column='median home value', cmap='OrRd', legend=True)
homeval_map
```

From the map, we can see that Manhattan and Western Brooklyn have the highest median house value. However, from our earlier exploration, we found that these areas also have high total complaints and high noise complaints. So, it's not necessarily true that higher house prices correlate with more friendly neighbors. Admittedly, the earlier suggestion was just a general one based on boroughs. However, it is possible that house prices within a borough may moderate the relationship between complaints and house values. To explore this possibility, we can look at scatter plots of complaint counts against median house values for each borough. This will allow us to see if there are any notable trends or patterns within each borough.

```{python}
### Plot total complaints against mean home value
comp_homeval_df = median_homeval_df.merge(total_comp_df, left_on='zipstr', right_on='zipstr').copy()
comp_homeval_df = comp_homeval_df.merge(comp_df[['zipstr', 'Borough']], on='zipstr')
comp_homeval_df
```

```{python}
(
    ggplot(comp_homeval_df, # The dataset we are using
        aes(x = 'median home value', y='total complaints', fill = 'Borough')) 
        + geom_point(size=3, stroke=0)
)
```

Based on the scatter plots for each borough, there doesn't seem to be a clear correlation between the number of complaints and house prices. This suggests that higher house prices may not necessarily lead to more friendly neighbors.

### Conclusion

In conclusion, analyzing complaint data can provide valuable insights for people looking to find a good place to live. By plotting complaint locations on a map, we can identify areas with higher complaint volumes and specific patterns or trends that may be unique to certain boroughs or neighborhoods. From the maps and analyses presented, we have identified some general areas that might be friendlier for different needs, such as Staten Island for those who are sensitive to noise or need to drive, or the Upper East Side for those who want a quieter location in Manhattan. However, when it comes to specific questions such as whether living near NYPD or paying higher house prices correlates with more friendly neighbors, the data does not provide clear evidence of any such correlation. Therefore, it is important to consider individual needs and preferences when selecting a neighborhood to live in.