---
title: "Midterm Project"
author: "Yang Kang Chua, Shen, Tong"
date: "March 3, 2023"
format:
  html:
    code-fold: true
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
jupyter: python3
---
## Data Cleaning
```{python}
import numpy as np
import pandas as pd

pd.set_option('display.max_columns', None)

nyc311 = pd.read_csv("nyc311_011523-012123_by022023.csv")

nyc311.head()

```

Get a statistical summary

```{python}
nyc311.describe()
```

Looks good! Lets take a look at the number of missing values.

```{python}
nyc311.isna().sum()
```

Get the type of the data frame
```{python}
nyc311.info();
```
Check missing value percentage

```{python}
# Find missing value
missing_value = nyc311.isnull()

#Divide the full data length to get the percentage
missing_value = missing_value.sum()/len(nyc311)

#Convert into dataframe
missing_value = missing_value.to_frame()

#Name the column
missing_value.columns = ['Missing Percentage']

#Display
missing_value

```
Convert the time to date time format.

```{python}
nyc311['Created Date'] = pd.to_datetime(nyc311['Created Date'], errors='coerce')
nyc311['Closed Date'] = pd.to_datetime(nyc311['Closed Date'], errors='coerce')
```
Let take a look on if `Closed Date` earlier than `Created Date`.

```{python}
# Create a filter to find if the Closed Date is earlier and Created Date

Filter_1 = nyc311[nyc311['Created Date'] > nyc311['Closed Date']]

Filter_1.head()
```

Few thing we can do to clean the data:

* Remove the `Created Date` and `Closed Date`.
* Missing Value from `Incident Zip` can be found using `Latitude` and `Longitude` using `uszipcode`.
* `Latitude` and `Longitude` can be found using the available `Incident Address`, `Street Name`, `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`, `Intersection Street 2` or as known as `Address Type`.
* We can get `X Coordinate (State Plane)`and `Y Coordinate (State Plane)` using `stateplane`.
* Update `Borough` from `Incident Zip`.

Begin by remove the `Created Date` and `Closed Date`.

```{python}
nyc311 = nyc311[nyc311['Created Date'] < nyc311['Closed Date']]

```

Now work on getting `Latitude` and `Longitude` can be found using the available `Incident Address`, `Street Name`, `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`, `Intersection Street 2` or as known as `Address Type`.

```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

pd.crosstab(Filter_1["Incident Address"].isnull(), Filter_1["Latitude"].isnull())
pd.crosstab(Filter_1["Street Name"].isnull(), Filter_1["Latitude"].isnull())
```

Begin with solving `Address Type` that is `INTERSECTION`.

```{python}
# Get the remaining data that is missing with Latitude
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

# Get the data that contain INTERSECTION
Filter_1 = Filter_1[Filter_1['Address Type'] =='INTERSECTION']

from geopy.geocoders import Nominatim
from geopy.geocoders import GoogleV3
from geopy.extra.rate_limiter import RateLimiter

Intersection = Filter_1['Intersection Street 1'] + ' @ ' + Filter_1['Intersection Street 2'] + ' NY'

for index in Intersection.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Intersection[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]
        
# Update the data
for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]

```
Then work with address type `BLOCKFACE`.
```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1 = Filter_1[Filter_1['Address Type'] == 'BLOCKFACE']

Cross_Street = Filter_1['Cross Street 1'] + ' @ ' + Filter_1['Cross Street 2'] + ' NY '

for index in Cross_Street.index:
    if not Filter_1['Cross Street 1'][index] == Filter_1['Cross Street 2'][index]:
        if Filter_1['Cross Street 2'][index] is not np.nan:
            if Filter_1['Cross Street 2'][index] != 'SEE COMMENTS FOR CROSS ST':
                geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
                location = geocode(Cross_Street[index], timeout = 10)
                if location is not None:
                    Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

# Use the Street Name and Cross Street 1 to get the latitude and longitude
Filter_2 = Filter_1[Filter_1['Latitude'].isnull()]

Street_Cross = Filter_2['Street Name'] + ' @ ' + Filter_2['Cross Street 1'] + ' NY '

for index in Street_Cross.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Street_Cross[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]
```

Work on address type `ADDRESS`.
```{python}
Filter_1 = nyc311[nyc311['Latitude'].isnull()]
Filter_1 = Filter_1[Filter_1['Address Type'] == 'ADDRESS']

Address = Filter_1['Incident Address'] + ' NY '

for index in Address.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Address[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

# Update
for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]


```
Solve the remaining address.
```{python}
# Solve the remaining address

Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1 = Filter_1[Filter_1['Incident Address'].notnull()]

Address = Filter_1['Incident Address'] + ' NY '

for index in Address.index:
    geocode = RateLimiter(GoogleV3(api_key='AIzaSyDhsJ5roz0w0ll0YegLJZn-niSBjm1ns5A').geocode, min_delay_seconds=1)
    location = geocode(Address[index], timeout = 10)
    if location is not None:
        Filter_1.loc[index,['Latitude',"Longitude"]] = [location.latitude,location.longitude]

for index in Filter_1.index:
    nyc311.loc[index,'Latitude']   = Filter_1['Latitude'][index]
    nyc311.loc[index,'Longitude'] = Filter_1['Longitude'][index]

```
These are the remaining latitude and longitude that is missing
```{python}
# Show the remaining data that has missing Latitude and Longitude

Filter_1 = nyc311[nyc311['Latitude'].isnull()]

Filter_1
```

Most of the latitude are due to most location is at hightway.

Now work on getting zipcode from available `latitude` and `longitude`.

```{python}
from uszipcode import SearchEngine

# Create a SearchEngine object
search = SearchEngine()

Filter_1 = nyc311[nyc311['Incident Zip'].isnull()]

Filter_1 = Filter_1[Filter_1['Latitude'].notnull()]

for index in Filter_1.index:
    z = search.by_coordinates(Filter_1['Latitude'][index],Filter_1['Longitude'][index], radius = 25)
    Filter_1.loc[index,'Incident Zip'] = z[0].zipcode

# Make an update
for index in Filter_1.index:
    nyc311.loc[index,'Incident Zip']   = Filter_1['Incident Zip'][index]

```

The remaining missing date still shown here, this means that the rest of the `Incident Zip` is filled!

```{python}
Filter_1 = nyc311[nyc311['Incident Zip'].isnull()]

Filter_1
```

Update `Borough` from `Incident Zip`.

```{python}
# Function that classified the borough
def nyczip2burough(zip):
    nzip = int(zip)
    if nzip >= 10001 and nzip <= 10282:
        return "MANHATTAN"
    elif nzip >= 10301 and nzip <= 10314:
        return "STATEN ISLAND"
    elif nzip >= 10451 and nzip <= 10475:
        return "BRONX"
    elif nzip >= 11004 and nzip <= 11109:
        return "QUEENS"
    elif nzip >= 11351 and nzip <= 11697:
        return "QUEENS"
    elif nzip >= 11201 and nzip <= 11256:
        return "BROOKLYN"
    else:
        return np.nan

Filter_1 = nyc311[nyc311['Borough'] == 'Unspecified']

Filter_1 = Filter_1[Filter_1['Incident Zip'].notnull()]

for index in Filter_1.index:
    burough = nyczip2burough(Filter_1.loc[index,['Incident Zip']])
    Filter_1.loc[index,'Borough'] = burough

#Update the data
for index in Filter_1.index:
  nyc311.loc[index,'Borough']   = Filter_1['Borough'][index]
```

Get `X Coordinate (State Plane)`and `Y Coordinate (State Plane)` using `stateplane`.

```{python}
import stateplane

Filter_1 = nyc311[nyc311['X Coordinate (State Plane)'].isnull()]

Filter_1 = Filter_1[Filter_1['Latitude'].notnull()]

for index in Filter_1.index:
    x, y = stateplane.from_lonlat(Filter_1['Longitude'][index],Filter_1['Latitude'][index])
    Filter_1.loc[index,'X Coordinate (State Plane)'] = x
    Filter_1.loc[index,'Y Coordinate (State Plane)'] = y

for index in Filter_1.index:
    nyc311.loc[index,'X Coordinate (State Plane)'] = Filter_1['X Coordinate (State Plane)'][index]
    nyc311.loc[index,'Y Coordinate (State Plane)'] = Filter_1['Y Coordinate (State Plane)'][index]

#Show the filled state plane 
Filter_1
```
### Summary after cleaning the data.

* There are more missing `X Coordinate (State Plane)` than `Y Coordinate (State Plane)`.
* There is duplicate when stating `Cross Street 1`, `Cross Street 2`, `Intersection Street 1`,and  `Intersection Street 2`.
* Agency from `DOT` seems commonly mixed up the `Closed Date` and `Created Date`.
* `Borough` that is `unspecified` should count as `missing`.
* Many `Park Facility Name` is `unspecified` should count as `missing`. 

## Generating new variable
Remove the request not from NYPD

```{python}
nyc311_NYPD = nyc311[nyc311['Agency'] == 'NYPD']

A = nyc311_NYPD[nyc311_NYPD['Closed Date'].isnull()]

A
```
Only one data still in progress. We can remove that too. Add a new variable called `Duration (hours)`

```{python}
nyc311_NYPD = nyc311_NYPD[nyc311_NYPD['Closed Date'].notnull()]

nyc311_NYPD['Duration (hours)'] = nyc311_NYPD['Closed Date'] - nyc311_NYPD['Created Date']

col = nyc311_NYPD.pop("Duration (hours)")

s_hours = col.dt.total_seconds() / 3600.0

nyc311_NYPD.insert(3,s_hours.name ,s_hours)

nyc311_NYPD['Day type'] = nyc311_NYPD['Created Date'].dt.dayofweek

for index in nyc311_NYPD.index:
    if nyc311_NYPD['Day type'][index] < 5:
       nyc311_NYPD.loc[index,'Day type'] = 'Weekday'
    else:
       nyc311_NYPD.loc[index,'Day type'] = 'Weekend'
       
col = nyc311_NYPD.pop("Day type")

nyc311_NYPD.insert(4,col.name ,col)

nyc311_NYPD
```

### Visualize the distribution of uncensored duration by weekdays/weekend and by borough.

Lets check the Borough that is `Unspecified`.

```{python}
# Remove the unspecified Borough
A = nyc311_NYPD[ nyc311_NYPD['Borough'] == 'Unspecified']

A
```

The remaining borough that is `Unspecified` does not have `Incident Zip` which merging with uszipcode does not help to get more info about the data. 

```{python}
# Remove the unspecified Borough
nyc311_NYPD = nyc311_NYPD[ nyc311_NYPD['Borough'] != 'Unspecified']

# Remove the zipcode that is null due to missing latitude and longitude
nyc311_NYPD = nyc311_NYPD[nyc311_NYPD['Incident Zip'].notnull()]

from plotnine import *

(
    ggplot(nyc311_NYPD, aes(x='Borough',y = 'Duration (hours)', fill='Day type'))
    + geom_boxplot()
)

```

Test whether the distributions are the same across weekdays/weekends.

```{python}
from scipy.stats import mannwhitneyu

# Create the subset of data
weekday_dur = nyc311_NYPD.loc[nyc311_NYPD['Day type'] == 'Weekday', 'Duration (hours)']

weekend_dur = nyc311_NYPD.loc[nyc311_NYPD['Day type'] == 'Weekend', 'Duration (hours)']

# Perform the weekday/weekend comparison
weekday_weekend_pvalue = mannwhitneyu(weekday_dur, weekend_dur).pvalue

print(weekday_weekend_pvalue)
```
The pvalue shows that is diffeent accross weekdays and weekends.

Test whether the distributions are the same across borough.

```{python}
# Create the subset of the data
from scipy.stats import kruskal

group1 =nyc311_NYPD[nyc311_NYPD['Borough'] == 'BROOKLYN']['Duration (hours)']
group2 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'QUEENS']['Duration (hours)']
group3 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'MANHATTAN']['Duration (hours)']
group4 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'BRONX']['Duration (hours)']
group5 = nyc311_NYPD[nyc311_NYPD['Borough'] == 'STATEN ISLAND']['Duration (hours)']

statistic, p_value = kruskal(group1, group2, group3, group4, group5)

print('Borough p-values:', p_value)
```
The pvalue shows that the distribution are different across across borough. 

Lets merge with uszipcode data.

```{python}
import os
# set the default database file location
db_file = os.path.abspath("simple_db.sqlite")

import sqlite3
import pandas as pd
# change to your own path after installing uszipcode
con = sqlite3.connect(db_file)
zipdf = pd.read_sql_query("SELECT * from simple_zipcode", con)
zipdf.info()

# Change the type to float64
zipdf['zipcode'] = zipdf['zipcode'].astype('float64')

# Change the type to float64 too
nyc311_NYPD['Incident Zip'] = nyc311_NYPD['Incident Zip'].astype('float64')

merged_df = pd.merge(nyc311_NYPD, zipdf, how = 'left', left_on = 'Incident Zip', right_on = 'zipcode')

# Drop repeating data
merged_df = merged_df.drop(['zipcode','lat','lng'],axis = 1)
```

Displaying merged data.

```{python}
merged_df

```

## Model Selection

### Importing new variable
Lets choose a model to predict `duration`.

```{python}
import pandas as pd
import numpy as np

# load data 
train = pd.read_csv("nyc311_NYPD_merged.csv")
test = pd.read_csv("nyc311_NYPD_test_merged.csv")

# Create new variable over3h.
train['over3h'] = train['Duration (hours)'].\
    apply(lambda x: 1 if x >= 3 else 0)
test['over3h'] = test['Duration (hours)'].\
    apply(lambda x: 1 if x >= 3 else 0)
```
Construct a `hour` variable with integer values from 0 to 23.
```{python}
train['hour'] = pd.to_datetime(train['Created Date']).dt.hour
test['hour'] = pd.to_datetime(test['Created Date']).dt.hour
```
Construct a `house_income_ratio` represent the ratio of `median_home_value` to `median_household_income`.
```{python}
train['house_income_ratio'] = train['median_home_value'] / train['median_household_income']
test['house_income_ratio'] = test['median_home_value'] / train['median_household_income']
```
Drop cases with missing values for certain columns.
```{python}
train = train.dropna(subset=['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type','population_density',\
                 'median_home_value', 'house_income_ratio'])
test = test.dropna(subset=['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type','population_density',\
                 'median_home_value', 'house_income_ratio'])
```

When using the fitted model to predict the test data, we noticed that there are certain features that appear only in the testing data (3 cases) and certain features that appear only in the training data (6 cases). Therefore, we need to revisit this step and drop these cases before proceeding with the fit and prediction.

Drop cases with certain values for certain columns

```{python}
test = test.drop(test[test['Community Board'] == '28 BRONX'].index)
test = test.drop(test[test['Community Board'] == '80 QUEENS'].index)
test = test.drop(test[test['Complaint Type'] == 'Squeegee'].index)
train = train.drop(train[train['Community Board'] == '27 BRONX'].index)
train = train.drop(train[train['Community Board'] == '81 QUEENS'].index)
train = train.drop(train[train['Resolution Description'] == \
'Your complaint has been received by the Police Department and additional information will be available later.'].index)
train = train.drop(train[train['Open Data Channel Type'] == 'UNKNOWN'].index)
```
Get the training data and separate the categorical and continuous variables into separate dataframes.
```{python}

train_categorical_variables = train[['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type']]
train_continuous_variables = train[['population_density', 'median_home_value', 'house_income_ratio']]

# Perform one-hot encoding on the categorical variables
train_categorical_encoded = pd.get_dummies(train_categorical_variables)
# Combine the one-hot encoded categorical variables with the continuous variables
X_train = pd.concat([train_categorical_encoded, train_continuous_variables], axis=1)
print(X_train.shape) 
```
Get the testing data and separate the categorical and continuous variables into separate dataframes
```{python}
test_categorical_variables = test[['hour', 'Day type', 'Complaint Type', 'Community Board',\
                 'Resolution Description', 'Open Data Channel Type']]
test_continuous_variables = test[['population_density', 'median_home_value', 'house_income_ratio']]

# Perform one-hot encoding on the categorical variables
test_categorical_encoded = pd.get_dummies(test_categorical_variables)

# Combine the one-hot encoded categorical variables with the continuous variables
X_test = pd.concat([test_categorical_encoded, test_continuous_variables], axis=1)

print(X_test.shape)
```

```{python}
# Get y for both the training and test data
y_train = train['over3h'].values
y_test = test['over3h'].values
```

### Import Model and fit the Model
```{python}
from sklearn.svm import SVC

# Fit SVM
svm = SVC()
svm.fit(X_train, y_train)
```

```{python}
from sklearn import tree

# Fit decision tree
tree1 = tree.DecisionTreeClassifier()
tree1.fit(X_train, y_train)
```

```{python}
tree1.tree_.node_count
```

```{python}
# caculate the predicted values
svm_pred = svm.predict(X_test)
tree1_pred = tree1.predict(X_test)
```

```{python}
# evaluate the model with default parameters
from sklearn.metrics import confusion_matrix, \
accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Confusion matrix
svm_cm = confusion_matrix(y_test, svm_pred)
tree1_cm = confusion_matrix(y_test, tree1_pred)

# Accuracy
svm_acc = accuracy_score(y_test, svm_pred)
tree1_acc = accuracy_score(y_test, tree1_pred)

# Precision
svm_precision = precision_score(y_test, svm_pred, zero_division=1)
tree1_precision = precision_score(y_test, tree1_pred)

# Recall
svm_recall = recall_score(y_test, svm_pred)
tree1_recall = recall_score(y_test, tree1_pred)

# F1-score
svm_f1 = f1_score(y_test, svm_pred)
tree1_f1 = f1_score(y_test, tree1_pred)

# AUC
svm_auc = roc_auc_score(y_test, svm_pred)
tree1_auc = roc_auc_score(y_test, tree1_pred)
```

```{python}
print("SVM results:")
print("Confusion matrix:")
print(svm_cm)
print("Accuracy:", svm_acc)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1-score:", svm_f1)
print("AUC:", svm_auc)

print("decision tree results:")
print("Confusion matrix:")
print(tree1_cm)
print("Accuracy:", tree1_acc)
print("Precision:", tree1_precision)
print("Recall:", tree1_recall)
print("F1-score:", tree1_f1)
print("AUC:", tree1_auc)
```

### Summary on SVM model
We observed that the SVM model performed poorly with the default parameters. To improve its performance, we decided to scale the continuous variables and use cross-validation to find the optimal hyperparameters. However, the SVM model took too long to train and did not converge even after running for half an hour.

In contrast, the decision tree model provided accurate predictions with the default parameters and was much more efficient than the SVM model. Therefore, we decided to use the decision tree model for further optimazition.

Here are the syntax that tried to optimize svm parameters
```python
from sklearn.preprocessing import StandardScaler

# scale the continuous variables 

scaler = StandardScaler()
X_train_cont_scaled = scaler.fit_transform(train_continuous_variables)
X_test_cont_scaled = scaler.transform(test_continuous_variables)
X_train_cont_scaled = pd.DataFrame(X_train_cont_scaled, columns=['scaled1', 'scaled2', 'scaled3'])
X_test_cont_scaled = pd.DataFrame(X_test_cont_scaled, columns=['scaled1', 'scaled2', 'scaled3'])
train_categorical_encoded = train_categorical_encoded.reset_index()
test_categorical_encoded = test_categorical_encoded.reset_index()
X_train_s = pd.concat([train_categorical_encoded, X_train_cont_scaled], axis=1)
X_test_s = pd.concat([test_categorical_encoded, X_test_cont_scaled], axis=1)

# Fit SVM with scaled variables
svm2 = SVC()
svm2.fit(X_train_s, y_train)

# define the hyperparameter space to search over for svm
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.001, 0.01, 0.1],
              'gamma': [0.01, 0.1, 1, 'scale', 'auto'],
              'kernel': ['linear', 'rbf', 'sigmoid']}

# perform cross-validation with GridSearchCV
grid_search = GridSearchCV(svm2, param_grid, cv=5, scoring='f1')

# fit the GridSearchCV object to the training data
grid_search.fit(X_train_s, y_train)

# print the best hyperparameters found
print("Best hyperparameters: ", grid_search.best_params_)
```

```{python}
# define the hyperparameter grid 

from sklearn.model_selection import GridSearchCV

param_grid = {'criterion': ['gini', 'entropy'],
              'min_impurity_decrease': [0, 1e-5, 1e-4, 1e-3],
              'ccp_alpha': [0.0, 1e-5, 1e-4, 1e-3, 0.01, 0.1]}

# perform cross-validation with GridSearchCV
grid_search = GridSearchCV(tree1, param_grid, cv=5, scoring='roc_auc')

# fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# print the best hyperparameters found
grid_search.best_params_
```

```{python}
# Use parameters from cross-validation to train another model
tree2 = tree.DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.001, min_impurity_decrease=0)
tree2 = tree2.fit(X_train, y_train)
tree2.tree_.node_count
```

```{python}
# caculate the predicted values
tree2_pred = tree2.predict(X_test)

# evaluate the model 

# Confusion matrix
tree2_cm = confusion_matrix(y_test, tree2_pred)

# Accuracy
tree2_acc = accuracy_score(y_test, tree2_pred)

# Precision
tree2_precision = precision_score(y_test, tree2_pred)

# Recall
tree2_recall = recall_score(y_test, tree2_pred)

# F1-score
tree2_f1 = f1_score(y_test, tree2_pred)

# AUC
tree2_auc = roc_auc_score(y_test, tree2_pred)
```

```{python}
print("decision tree2 results:")
print("Confusion matrix:")
print(tree2_cm)
print("Accuracy:", tree2_acc)
print("Precision:", tree2_precision)
print("Recall:", tree2_recall)
print("F1-score:", tree2_f1)
print("AUC:", tree2_auc)
```

### Summary for Model Selection
Cross-validation was used to optimize the tuning parameters for building the second decision tree model with the objective of minimizing overfitting and maximizing AUC. While the second model (tree2) performed better on accuracy and precision, it did not perform as well as the first model (tree1) on other metrics such as recall, F1 score, and AUC. However, tree2 had significantly fewer nodes than tree1. Overall, both models performed adequately and the choice of model would depend on the specific metric that is more important for the task at hand. For instance, if higher precision is desired, then tree1 would be the preferred choice.